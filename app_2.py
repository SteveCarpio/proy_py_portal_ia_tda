# ==========================================
# ðŸ§  LocalDocChat - RAG con Memoria y Ollama
# ==========================================
# Compatible con LangChain 1.0+, Streamlit y Ollama
# Autor: ChatGPT (versiÃ³n optimizada y documentada)
# ------------------------------------------

import streamlit as st
import os

# --- LangChain imports (estructura 2025) ---
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings.ollama import OllamaEmbeddings
from langchain_community.llms.ollama import Ollama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate

# --- ConfiguraciÃ³n del modelo y directorios ---
OLLAMA_MODEL = "llama3"   # Cambia por el modelo Ollama que tengas (mistral, phi3, etc.)
CHROMA_PATH = "chroma_db" # Carpeta donde se guardarÃ¡ la base vectorial

# ------------------------------------------
# ðŸ”§ FunciÃ³n: Procesar e indexar un PDF
# ------------------------------------------
def process_and_index_pdf(file_path):
    """
    Carga un PDF, lo divide en fragmentos (chunks),
    genera embeddings con Ollama y los guarda en Chroma.
    """

    loader = PyPDFLoader(file_path)
    documents = loader.load()
    st.success(f"âœ… Documento cargado ({len(documents)} pÃ¡ginas).")

    # DivisiÃ³n del texto en fragmentos para mejorar la bÃºsqueda semÃ¡ntica
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,     # tamaÃ±o de fragmento
        chunk_overlap=200,    # solapamiento entre fragmentos
        length_function=len,
    )
    chunks = text_splitter.split_documents(documents)
    st.info(f"ðŸ“‘ Documento dividido en {len(chunks)} fragmentos.")

    # Crear embeddings con modelo de Ollama
    embeddings = OllamaEmbeddings(model=OLLAMA_MODEL)

    # Crear base vectorial Chroma
    vectorstore = Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        persist_directory=CHROMA_PATH
    )
    #vectorstore.persist()
    st.success("ðŸ“š Documento indexado y guardado en Chroma.")
    return vectorstore

# ------------------------------------------
# ðŸ§© FunciÃ³n: Crear la cadena RAG con memoria
# ------------------------------------------
def create_rag_chain(vectorstore):
    """
    Crea una funciÃ³n que toma una pregunta, recupera contexto relevante
    del PDF y genera una respuesta usando Ollama.
    Incluye memoria conversacional basada en el historial de chat.
    """

    retriever = vectorstore.as_retriever()
    llm = Ollama(model=OLLAMA_MODEL)

    # Prompt que guÃ­a al modelo
    prompt_template = ChatPromptTemplate.from_template(
        """
        Eres un asistente que responde basÃ¡ndote solo en el contexto dado.
        Si la informaciÃ³n no estÃ¡ en el contexto, di claramente:
        "No encontrÃ© esa informaciÃ³n en el documento."

        Ten en cuenta la conversaciÃ³n previa si ayuda a entender la pregunta.

        ----
        Historial del chat:
        {history}

        Contexto relevante del documento:
        {context}

        Pregunta actual:
        {question}
        """
    )

    def rag_query(question, history):
        """Ejecuta una consulta RAG teniendo en cuenta el historial."""
        # Recupera documentos relevantes
        docs = retriever.invoke(question)
        context = "\n\n".join([doc.page_content for doc in docs])

        # Prepara el historial como texto legible
        history_text = "\n".join([f"{h['role']}: {h['content']}" for h in history[-5:]])  # Ãºltimas 5 interacciones

        # Rellena el prompt
        prompt = prompt_template.format(context=context, question=question, history=history_text)

        # Genera la respuesta con el LLM
        response = llm.invoke(prompt)
        return str(response)

    return rag_query

# ------------------------------------------
# ðŸ’¬ Interfaz principal Streamlit
# ------------------------------------------
st.set_page_config(page_title="LocalDocChat - RAG con Memoria", layout="wide")
st.title("ðŸ’¬ LocalDocChat (RAG con Ollama y Streamlit)")
st.caption("Tu asistente local para consultar documentos PDF. Compatible con Ollama y Chroma.")

# --- Subida del archivo PDF ---
uploaded_file = st.sidebar.file_uploader("ðŸ“„ Sube un archivo PDF para analizar", type="pdf", key="pdf_uploader")

if uploaded_file is not None:
    # Guardar PDF en carpeta local
    if not os.path.exists("data"):
        os.makedirs("data")

    file_path = os.path.join("data", uploaded_file.name)
    with open(file_path, "wb") as f:
        f.write(uploaded_file.getbuffer())

    st.sidebar.success(f"âœ… Archivo guardado: {uploaded_file.name}")

    with st.spinner("ðŸ§  Procesando e indexando el documento..."):
        vector_store = process_and_index_pdf(file_path)
        # Guardar en sesiÃ³n para mantener persistencia
        st.session_state['vector_store'] = vector_store
        st.session_state['chat_history'] = []
        
        #st.experimental_rerun()


# --- Zona de Chat ---
if 'vector_store' in st.session_state:
    st.header("ðŸ’­ Chatea con tu Documento")

    # Inicializar memoria si no existe
    if 'chat_history' not in st.session_state:
        st.session_state['chat_history'] = []

    rag_chain = create_rag_chain(st.session_state['vector_store'])

    # Mostrar historial del chat
    for msg in st.session_state['chat_history']:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Entrada del usuario
    if prompt := st.chat_input("Haz una pregunta sobre el documento..."):
        # AÃ±adir pregunta al historial
        st.session_state['chat_history'].append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ðŸ§© Buscando en el documento y generando respuesta..."):
                response = rag_chain(prompt, st.session_state['chat_history'])
                st.markdown(response)

            # Guardar respuesta en el historial
            st.session_state['chat_history'].append({"role": "assistant", "content": response})
else:
    st.info("ðŸ‘ˆ Sube un archivo PDF en la barra lateral para comenzar.")

# --- InformaciÃ³n en la barra lateral ---
st.sidebar.markdown("---")
st.sidebar.markdown(f"ðŸ§© Modelo LLM: **{OLLAMA_MODEL}**")
st.sidebar.markdown(f"ðŸ“‚ Base de datos Chroma: `{CHROMA_PATH}`")
st.sidebar.markdown("ðŸ’¡ Consejo: Usa preguntas especÃ­ficas para obtener mejores respuestas.")
